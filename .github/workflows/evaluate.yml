name: Evaluate Submission

on:
  pull_request:
    paths:
      - 'solution.py'

jobs:
  evaluate:
    runs-on: ubuntu-latest
    permissions:
      contents: read

    steps:
    - name: Checkout code
      uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pygame numpy opencv-python

    - name: Run Test Runner
      id: test
      run: |
        # Run the headless test runner
        # Capture output. If successful, it prints the time.
        export PYGAME_HIDE_SUPPORT_PROMPT=1
        # Run the simulation and capture all output
        OUTPUT=$(python main.py --test)
        EXIT_CODE=$?
        
        # Print output for debugging
        echo "$OUTPUT"
        
        if [ $EXIT_CODE -ne 0 ]; then
          echo "Simulation Failed!"
          exit 1
        fi
        
        # Extract the score using grep (looks for "FINAL_SCORE: 12.3456")
        SCORE=$(echo "$OUTPUT" | grep "FINAL_SCORE:" | cut -d ' ' -f 2)
        
        if [ -z "$SCORE" ]; then
          echo "Error: Could not find FINAL_SCORE in output"
          exit 1
        fi
        
        echo "Simulation Success! Time: $SCORE"
        
        # Save score and user info to a file for the next workflow
        mkdir -p artifacts
        echo "$SCORE" > artifacts/score.txt
        echo "${{ github.event.pull_request.user.login }}" > artifacts/user.txt
        echo "${{ github.event.pull_request.html_url }}" > artifacts/pr_url.txt
        echo "${{ github.base_ref }}" > artifacts/branch.txt

    - name: Upload Score Artifact
      uses: actions/upload-artifact@v4
      with:
        name: competition-results
        path: artifacts/
